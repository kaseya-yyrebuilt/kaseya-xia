<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Kaseya Xia (Ziyi Xia)</title>
  
  <meta name="author" content="Kaseya Xia (Ziyi Xia)">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Kaseya Xia</name>
              </p>
              <p>I am a second year Ph.D student in Computer Science at the <a href="https://www.cs.ubc.ca/">University of British Columbia (UBC)</a>, where I am supervised by Dr. <a href="https://www.robertxiao.ca/">Robert Xiao</a> and Dr. <a href="https://ece.ubc.ca/sid-fels/">Sidney Fels</a>. I am currently looking for internship!
              <p>
                Previously, I have completed my MASc. at the University of British Columbia and dual B.Sc. in Electrical Engineering and Biomedical Engineering at the <a href="https://www.uvm.edu/">University of Vermont</a>, where I participated in research in Dr. <a href=" https://comis.med.uvm.edu/bioviewer/WebBio.aspx?BioID=22708"> Daneil Weiss</a>'s Lab and Dr.<a href=" https://www.uvm.edu/cems/ebe/profiles/ryan_mcginnis
                "> Ryan McGinnis</a>'s Lab.
              </p>
              <p style="text-align:center">
                <a href="mailto:zxia@ece.ubc.ca">Email</a> &nbsp/&nbsp
                <a href="data/CV-Kaseya Xia.pdf">CV</a> &nbsp/&nbsp       
                <a href="https://github.com/kaseya-yyrebuilt">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/kaseya-xia-772057b5/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="img/Kaseya_Circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="img/Kaseya_Circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My current research interests focus on HCI (Human-Computer-Interaction), CV (Computer Vision) and VR (Virtual Reality). Recently, I am also passionate about machine learning and fascinated by GAN and reinforcement learning in particular. My research goal is to create photorealistic rendering humans and natural looking objects in the VR world or augmented reality.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <a href="img/TeleViewDemo-pic.png"><img style="width:100%;max-width:100%" alt="profile photo" src="img/TeleViewDemo-pic.png" class="hoverZoomLink"></a>
          </td>
        
          <td style="padding:20px;width:75%;vertical-align:middle">              
            <papertitle><font color=#FF000></font>TeleViewDemo: Experience the Future of 3D Teleconferencing</papertitle>
            </a>
            <br>
            <strong>Ziyi Xia</strong>,
            <a>Frank Yu</a>, <a>Beibei Xiong</a>, <a>Emily Jia</a>, <a>Seungyeon Baek</a>, <a>James Gregson</a>, <a>Xingzhe He</a>,
            <a href="https://www.cs.ubc.ca/~rhodin/">Helge Rhodin</a>,
            <a href="https://ece.ubc.ca/sid-fels/">Sidney Fels</a>,
            <br>
            <em><font color=#FF8080><strong>SIGGRAPH Asia 2022</strong></font></em> (XR)
            <br>
            <a href="https://dl.acm.org/doi/10.1145/3550472.3558404">Paper</a> | <a href="https://hct.ece.ubc.ca/openteleview/">Project Page</a>
            <p></p>
            <p>
              Our platform lets visitors experience real-time end-to- end 3D teleconferencing using commodity hardware. This demo integrates current state-of-the-art face reconstruction and render- ing algorithms into an end-to-end system to illustrate the utility and capabilities of commodity telepresence on a range of 3D displays.
            </p>
          </td>
        <!-- HALOTOUCH – CHI 2025 --> 
         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <td style="padding:20px;width:25%;vertical-align:middle">
             <a href="img/halotouch.png"><img style="width:100%;max-width:100%" alt="HaloTouch teaser" src="img/halotouch.png" class="hoverZoomLink"></a> 
            </td> 
            
            <td style="padding:20px;width:75%;vertical-align:middle"> 
              <papertitle><font color=#FF0000>HaloTouch: Using IR Multi-Path Interference to Support Touch Interactions with General Surfaces</font></papertitle>
              <br> <strong>Ziyi Xia</strong>, <a>Xincheng Huang</a>, <a>Sidney S. Fels</a>, <a>Robert Xiao</a>
              <br> <em><font color=#FF8080><strong>CHI 2025</strong></font></em>
              <br> <a href="https://dl.acm.org/doi/10.1145/3706598.3714179">Paper</a> | <a href="https://www.robertxiao.ca/research/halotouch/">Project Page</a> | <a href="https://arxiv.org/abs/2503.01197">arXiv</a> 
              <p></p> 
              <p> Vision-based touch sensing on arbitrary surfaces using commodity depth cameras by leveraging IR multipath “halo” signals; demonstrates high-accuracy touch, hover, and pressure input without surface/user instrumentation and validates with a 12-participant study. :contentReference[oaicite:0]{index=0} 
              </p> 
            </td> 
          </tbody></table>
        

        
          
        </tr>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <a href="img/Gait-pic.png"><img style="width:100%;max-width:100%" alt="profile photo" src="img/Gait-pic.png" class="hoverZoomLink"></a>
          </td>
        
          <td style="padding:20px;width:75%;vertical-align:middle">              
            <papertitle> Gait Kinematics and Muscle Activity from Wearable Sensors Associated with Disability in Persons with Multiple Sclerosis</papertitle>
            </a>
            <br>
            <strong>Kaseya Xia</strong>,
            <a>Lara Weed</a>, <a>Lukas Adamowicz</a>, <a>Caroline Duksta</a>, <a>Gianna Barnhart</a>, <a>Andrew J. Solomon</a>, <a>Ryan S. McGinnis</a>
            <br>
            <em><font color=#FF8080><strong>BMES 2018</strong></font></em> (Poster)
            <br>
            <a href="data/Gait-abstract.pdf">Abstract</a> | <a href="data/Gait-poster.pdf"> Poster </a>
            <p></p>
            <p>
              With the advent of new wearable sensors, there is an interest in developing new approaches for objectively quantifying symptoms in Multiple Sclerosis (MS) so as to improving tracking of their progression and response to intervention. Here we examine the use of a novel, multi- modal wearable sensor for capturing gait characteristics associated with MS-induced disability and mobility impairment.
            </p>
          </td>
          
        </tr> 
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <a href="img/cytotoxicity-pic.png"><img style="width:100%;max-width:100%" alt="profile photo" src="img/cytotoxicity-pic.png" class="hoverZoomLink"></a>
          </td>
        
          <td style="padding:20px;width:75%;vertical-align:middle">              
            <papertitle>Investigating Photo-Initiator Cytotoxicity in a ECM-Polymer Composite Substrate with Tunable Mechanical Properties</papertitle>
            </a>
            <br>
            <a>John Ethan</a>,
            <strong>Kaseya Xia</strong>,
            <a>Alison Kahn</a>,<a>Jessica Peura</a>, <a>Robert Pouliot</a>, <a>Daniel Weiss</a>,
            <br>
            <em><font color=#FF8080><strong>BMES 2018</strong></font></em> (Poster)
            <br>
            <a href="data/cytotoxicity-abstract.pdf">Abstract</a> | <a href="data/cytotoxity-poster.pdf"> Poster </a>
            <p></p>
            <p>The purpose of this work was to identify a photo-crosslinkable hydrogel containing decellularized porcine lung ECM and methacrylated alginate that can support human derived lung derived epithelial cells and mesenchymal stromal cells. The cytotoxicity of Eosin Y/NVP/TEAO was compared that of LAP to determine the feasibility of each photo initiator system.
            </p>
          </td> -->
          
        </tr>     
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Projects</heading>
            <p>
            I worked on a diverse kinds of engineering projects arranging from hardware to software since undergrad. I currently focus more on software development such as Webapp, Unity games, Artificial intelligence, and HCI.
            </p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <a href="img/metahire-pic.png"><img style="width:100%;max-width:100%" alt="profile photo" src="img/metahire-pic.png" class="hoverZoomLink"></a>
        </td>
      
        <td style="padding:20px;width:75%;vertical-align:middle">              
          <papertitle><font color=#FF000></font>Metahire: A more fair hiring tool</papertitle>
          </a>
          <br>
          <a>Wei Ning</a>, <strong>Kaseya Xia</strong>, <a>Jialiang</a>, <a>Jie Chen</a>
          <br>
          <em><font color=#FF8080><strong>Intership Project</strong></font></em> (2022 Hirebeat)
          <br>
          <a href="https://kaseya-yyrebuilt.itch.io/metahire">Beta Version Demo</a>
          <p></p>
          <p>
            This project introduces an online hiring tool that separates participants’ identity and the qualification employers are looking for so that participants will not be biased due to their race, gender, name, etc. The platform creates a networking environment in a game setting and also lets users simulate interview process and upload resume for review.
          </p>
        </td>
        
      </tr>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <a href="img/Ecog-pic.png"><img style="width:100%;max-width:100%" alt="profile photo" src="img/Ecog-pic.png" class="hoverZoomLink"></a>
        </td>
      
        <td style="padding:20px;width:75%;vertical-align:middle">              
          <papertitle><font color=#FF000></font>ECoG-GAN: Data Augmentation for Electrocorticography</papertitle>
          </a>
          <br>
          <strong>Kaseya Xia</strong>,
          <a>Yibo Jiao</a>
          <br>
          <em><font color=#FF8080><strong>CPSC554X</strong></font></em> (2021 Fall)
          <br>
          <a href="data/Ecog-paper.pdf">Paper</a> | <a href="https://github.com/kaseya-yyrebuilt/EcoGGAN"> Code </a>
          <p></p>
          <p>
            This project introduces data augmentation on electro-corticography (ECoG) using General Adversarial Networks (GANs). We provide statistical evaluations on the quality of generated ECoG data by some state-of-the-art GANs. We also present our own novel attentive context normalization GAN (ACNGAN) with additional speech classification evaluation.
          </p>
        </td>
        
      </tr>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <a href="img/Gazer-pic.png"><img style="width:100%;max-width:100%" alt="profile photo" src="img/Gazer-pic.png" class="hoverZoomLink"></a>
        </td>
      
        <td style="padding:20px;width:75%;vertical-align:middle">              
          <papertitle> Are You Gazing at Me? Eye Contact and Mini Virtual Room in Web Video Conferencing</papertitle>
          </a>
          <br>
          <strong>Kaseya Xia</strong>,
          <a>Beibei Xiong</a>
          <br>
          <em><font color=#FF8080><strong>CPSC 554</strong></font></em> (2021 Spring)
          <br>
          <a href="https://youtu.be/Gs97BQO6E7w">Video</a> | <a href="data/Gazer-paper.pdf"> Paper </a> |<a href="https://github.com/kaseya-yyrebuilt/VirtualGazer"> Code </a>
          <p></p>
          <p>
            This project introduces a new way to achieve eye contact for multi-person teleconferencing. Our WVC prototype, VirtualGazer, simulates eye contact with a 2D eye model and a mini-virtual-room VR model. We conducted empirical user study and semi-structured interviews to investigate how including eye contact in current WVC systems affects users’ online meeting experience. We found that eye contact can enrich interactive experiences, and enhance engagement level and focus level. Our VR model is found to generally attract more attention from users than the eye model. 
          </p>
        </td>
        
      </tr> 
      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <a href="img/Face-pic.png"><img style="width:100%;max-width:100%" alt="profile photo" src="img/Face-pic.png" class="hoverZoomLink"></a>
        </td>
      
        <td style="padding:20px;width:75%;vertical-align:middle">              
          <papertitle>Face Detection and Recognition for Fish- Tank Virtual Reality by Deep Network</papertitle>
          </a>
          <br>
          <strong>Kaseya Xia</strong>
          <br>
          <em><font color=#FF8080><strong>EECE 570</strong></font></em> (2020 Fall)
          <br>
          <a href="data/Face-paper.pdf">Paper</a> | <a href="https://github.com/kaseya-yyrebuilt/FaceDetection-FTVR"> Code </a>
          <p></p>
          <p>This project proposes a face detection network based on the Multi-task Convolutional Neural Network which leverages the depth map detection to increase detection robustness. For face recognition, we perform transfer learning on 3 user input videos on VGG-16 network to mimic the FTVR user on site experience. Our system showed robustness to detection distance and invariance to illumination with RGB input. However, the depth detection network and the detection speed still need improvement. The face recognition demonstrates 80% accurate rate for single user but higher false negative rate for two user cases.
          </p>
        </td>
        
      </tr>  
      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <a href="img/Drone-pic.png"><img style="width:100%;max-width:100%" alt="profile photo" src="img/Drone-pic.png" class="hoverZoomLink"></a>
        </td>
      
        <td style="padding:20px;width:75%;vertical-align:middle">              
          <papertitle>UAV and Lora Network Based Lake Champlain Water Quality Sensing and Mapping</papertitle>
          </a>
          <br>
          <strong>Kaseya Xia</strong>,
          <a>Nathaniel Cassidy </a>, <a>Abdoulaye Ira</a>, <a>Andrew Knowlton</a>
          <br>
          <em><font color=#FF8080><strong>Engineering Capstone Project</strong></font></em> (2019-2020)
          <br>
          <a href="data/Drone-paper.pdf">Paper</a> | <a href="data/Drone-poster.pdf"> Poster </a> | <a href="https://github.com/kaseya-yyrebuilt/UAV"> Code </a> 
          <p></p>
          <p>Currently the sampling of lake water quality at Lake Champlain is performed by people using boats to collect and test water samples. It can take up to two days for sample analyses to be completed, so the worst of conditions often pass before results are available. This project proposes a design a water sampling system to test and provide real time mapping  of the Lake Champlain water quality.
          </p>
        </td>
        
      </tr>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Mini Projects</heading>
          <p>
          I am also a student activity enthusiast. I believe the development of technology should find a purpose in serving people and educating next generation. I was the <a href="https://www.uvm.edu/~ieee/index.html"> UVM IEEE </a> student club president 2019-2020 and oversaw multiple electronics projects. 
          </p>
        </td>
      </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <a href="img/pumpkin.gif"><img style="width:100%;max-width:100%" alt="profile photo" src="img/pumpkin.gif" class="hoverZoomLink"></a>
      </td>
    
      <td style="padding:20px;width:75%;vertical-align:middle">              
        <papertitle><font color=#FF000></font>Lighting Pumpkin Curving</papertitle>
        </a>
        <br>
        <em><font color=#FF8080><strong>IEEE Event</strong></font></em>
        <br>
        <a href="https://drive.google.com/drive/folders/1P0ib5AKdT4nfSjv39YxwuQN_qqMTFfno?usp=sharing">Pictures</a> | <a href="https://github.com/kaseya-yyrebuilt/lighting-pumpkin">Code</a>
        <p></p>
        <p>
          This project uses Arduino nano and LED strips that are programmed with different patterns to light up pumpkin.
        </p>
      </td>
      
    </tr>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <a href="img/hand-video.gif"><img style="width:100%;max-width:100%" alt="profile photo" src="img/hand-video.gif" class="hoverZoomLink"></a>
      </td>
    
      <td style="padding:20px;width:75%;vertical-align:middle">              
        <papertitle><font color=#FF000></font>Hand Tracking Volume Controller using webcam</papertitle>
        </a>
        <br>
        <em><font color=#FF8080><strong>Side Project</strong></font></em>
        <br>
        <a href="https://github.com/kaseya-yyrebuilt/HandTracking-Vol-Controller"> Code </a>
        <p></p>
        <p>
          This project uses the mediapipe library to perform hand tracking and python to create a UI interface for drawing on canvasand for controling computer sound volume.  
        </p>
      </td>
      
    </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Teaching</heading>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <a href="img/ubc.png"><img style="width:100%;max-width:100%" alt="profile photo" src="img/ubc.png" class="hoverZoomLink"></a>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">              
          <papertitle> TA for ELEC 203: Basic Circuit Analysis [2021 Fall]</papertitle>
          <br>
          Instructor: Dr. <a href="https://ece.ubc.ca/saloome-motavas/">Saloome Motavas</a>
          <br>
          <p>
            Led weekly tutorials, lab sessions and office hours. Assisted with assignment and final exam grading.
          </p>
        </td>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <a href="img/ubc.png"><img style="width:100%;max-width:100%" alt="profile photo" src="img/ubc.png" class="hoverZoomLink"></a>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">              
          <papertitle> TA for ELEC 202: Circuit Analysis II [2022 Spring]</papertitle>
          <br>
          Instructor: Dr. <a href="https://ece.ubc.ca/l-r-linares/">Luis Linares</a>
          <br>
          <p>
            Assisted with assignment and final exam grading.
          </p>
        </td>
        
      </tr> 
    
           
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Credits to <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a> for the website design.
               

              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
